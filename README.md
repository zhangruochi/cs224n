# [CS224n: Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/)


## Introduction and Word Vectors 
- [x] video 
- [x] slide
- [x] notes
- [x] Gensim word vectors example
- [x] Assignment 1
- Suggested Readings
    - [x] Word2Vec Tutorial - The Skip-Gram Model
    - [ ] Efficient Estimation of Word Representations in Vector Space
    - [ ] Distributed Representations of Words and Phrases and their Compositionality 


## Word Vectors 2 and Word Senses
- [x] video 
- [x] slide
- [x] notes
- Suggested Readings
    - [ ] GloVe: Global Vectors for Word Representation
    - [ ] Improving Distributional Similarity with Lessons Learned from Word Embeddings
    - [ ] Evaluation methods for unsupervised word embeddings
    - [ ] A Latent Variable Model Approach to PMI-based Word Embeddings
    - [ ] Linear Algebraic Structure of Word Senses, with Applications to Polysemy
    - [ ] On the Dimensionality of Word Embedding.
- [x] [Review](https://zhangruochi.com/Word-Vectors/2019/12/04/)

## Python review session 
- [x] slide

## Word Window Classification, Neural Networks, and Matrix Calculus 
- [x] video 
- [x] slide
- [x] notes
- [x] Assignment 2
- Suggested Readings
    - [x] CS231n notes on backprop
    - [ ] Efficient Estimation of Word Representations in Vector Space
    - [ ] Distributed Representations of Words and Phrases and their Compositionality 


## Backpropagation and Computation Graphs 
- [x] video 
- [x] slide
- [x] notes
- Suggested Readings
    - [x] CS231n notes on network architectures
    - [ ] Learning Representations by Backpropagating Errors
    - [x] Derivatives, Backpropagation, and Vectorization
    - [x] Yes you should understand backprop
- [x] [Review](https://zhangruochi.com/Computational-Graph/2019/12/06/)

## Linguistic Structure: Dependency Parsing 
- [x] video 
- [x] slide
- [x] notes
- [x] Assignment 3
- Suggested Readings
    - [ ] Incrementality in Deterministic Dependency Parsing
    - [ ] A Fast and Accurate Dependency Parser using Neural Networks Dependency Parsing
    - [ ] Globally Normalized Transition-Based Neural Networks
    - [ ] Universal Stanford Dependencies: A cross-linguistic typology
    - [ ] Universal Dependencies website
- [x] [Review](https://zhangruochi.com/Dependency-Parsing-and-Assignment3-of-CS224n/2019/12/10/)


## Recurrent Neural Networks and Language Models 
- [x] video 
- [x] slide
- [ ] notes
- Suggested Readings
    - [ ] N-gram Language Models (textbook chapter)
    - [ ] The Unreasonable Effectiveness of Recurrent Neural Networks (blog post overview)
    - [ ] Sequence Modeling: Recurrent and Recursive Neural Nets (Sections 10.1 and 10.2)
    - [ ] On Chomsky and the Two Cultures of Statistical Learning


## Vanishing Gradients and Fancy RNNs  
- [x] video 
- [x] slide
- [x] notes
- Suggested Readings
    - [ ] Sequence Modeling: Recurrent and Recursive Neural Nets (Sections 10.3, 10.5, 10.7-10.12)
    - [ ] Learning long-term dependencies with gradient descent is difficult (one of the original vanishing gradient papers)
    - [ ] On the difficulty of training Recurrent Neural Networks (proof of vanishing gradient problem)
    - [ ] Vanishing Gradients Jupyter Notebook (demo for feedforward networks)
    - [ ] Understanding LSTM Networks (blog post overview)


    