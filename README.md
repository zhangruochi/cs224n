# [CS224n: Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/)


## Introduction and Word Vectors 
- [x] video 
- [x] slide
- [x] notes
- [x] Gensim word vectors example
- [x] Assignment 1
- Suggested Readings
    - [x] Word2Vec Tutorial - The Skip-Gram Model
    - [ ] Efficient Estimation of Word Representations in Vector Space
    - [ ] Distributed Representations of Words and Phrases and their Compositionality 


## Word Vectors 2 and Word Senses
- [x] video 
- [x] slide
- [x] notes
- Suggested Readings
    - [ ] GloVe: Global Vectors for Word Representation
    - [ ] Improving Distributional Similarity with Lessons Learned from Word Embeddings
    - [ ] Evaluation methods for unsupervised word embeddings
    - [ ] A Latent Variable Model Approach to PMI-based Word Embeddings
    - [ ] Linear Algebraic Structure of Word Senses, with Applications to Polysemy
    - [ ] On the Dimensionality of Word Embedding.
- [x] [review](https://zhangruochi.com/Word-Vectors/2019/12/04/)

## Python review session 
- [x] slide

## Word Window Classification, Neural Networks, and Matrix Calculus 
- [x] video 
- [x] slide
- [x] notes
- [x] Assignment 2
- Suggested Readings
    - [x] CS231n notes on backprop
    - [ ] Efficient Estimation of Word Representations in Vector Space
    - [ ] Distributed Representations of Words and Phrases and their Compositionality 


## Backpropagation and Computation Graphs 
- [x] video 
- [x] slide
- [x] notes
- Suggested Readings
    - [x] CS231n notes on network architectures
    - [ ] Learning Representations by Backpropagating Errors
    - [x] Derivatives, Backpropagation, and Vectorization
    - [x] Yes you should understand backprop
- [x] [review](https://zhangruochi.com/Computational-Graph/2019/12/06/)
